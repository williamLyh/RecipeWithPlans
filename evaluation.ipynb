{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import spacy\n",
    "import re\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from evaluation_utils import evaluate_fluency, compute_stage_matching, calculate_ingredient_coverage_and_hallucination\n",
    "from evaluation_utils import RecipeInferenceDataset, extract_instruction\n",
    "from dataclass import load_tokenizer\n",
    "from generator import RecipeGenerator\n",
    "from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "\n",
    "# from transformers import GPT2LMHeadModel, GPT2Tokenizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(121084, 121084)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "planner_result_path='/home/yinhong/Documents/source/RecipeWithPlans/model-checkpoint/planner_results/'\n",
    "with open(planner_result_path+'planner_prediction_test.json') as f:\n",
    "    test_predicted_plan = json.load(f)['planner_prediction']\n",
    "\n",
    "test_data_path='/home/yinhong/Documents/datasets/recipe1m+/preprocessed_data/test_dataset.json'\n",
    "with open(test_data_path) as f:\n",
    "    test_data = json.load(f)\n",
    "test_data, test_plan = test_data['text'], test_data['stage_label']\n",
    "\n",
    "len(test_predicted_plan), len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_path = '/home/yinhong/Documents/source/RecipeWithPlans/model-checkpoint/generator_results/checkpoint-90000'\n",
    "classifier_path = '/home/yinhong/Documents/source/RecipeWithPlans/model-checkpoint/classifier_results/checkpoint-45000'\n",
    "planner_path = '/home/yinhong/Documents/source/RecipeWithPlans/model-checkpoint/planner_results/checkpoint-8000'\n",
    "\n",
    "# Load device\n",
    "device = torch.device('cuda')\n",
    "\n",
    "# Load tokenizer \n",
    "tokenizer = load_tokenizer(generator_path)\n",
    "\n",
    "# # Load planner\n",
    "# planner_model = BartForConditionalGeneration.from_pretrained(planner_path)\n",
    "# bart_tok = BartTokenizer.from_pretrained(planner_path)\n",
    "\n",
    "# Load generator\n",
    "generator_model = RecipeGenerator(generator_path, \n",
    "                                tokenizer=tokenizer, \n",
    "                                device=device, \n",
    "                                classifier_path=classifier_path\n",
    "                                )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate classifier and planner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exact_match(predict_seq, reference_seq):\n",
    "    match_cnt = 0\n",
    "    total_cnt = 0\n",
    "    for predicted_plan, reference_plan in zip(predict_seq, reference_seq):\n",
    "        for p1, p2 in zip(predicted_plan, reference_plan):\n",
    "            if p1 and p2 and p1==p2:\n",
    "                match_cnt += 1\n",
    "\n",
    "        total_cnt += len(predicted_plan)\n",
    "    return match_cnt/total_cnt\n",
    "\n",
    "def plan_to_unigram(plan):\n",
    "    return [(stage) for stage in plan]\n",
    "    \n",
    "def plan_to_bigram(plan):\n",
    "    result = []\n",
    "    for i in range(len(plan)-1):\n",
    "        result.append(tuple(plan[i:i+2]))\n",
    "    return result\n",
    "\n",
    "def plan_to_trigram(plan):\n",
    "    result = []\n",
    "    for i in range(len(plan)-2):\n",
    "        result.append(tuple(plan[i:i+3]))\n",
    "    return result\n",
    "\n",
    "\n",
    "def n_gram_match_rate(predict_seq, reference_seq, ngram=1):\n",
    "    if ngram==1:\n",
    "        reference_ngram = [plan_to_unigram(plan) for plan in reference_seq]\n",
    "        prediction_ngram = [plan_to_unigram(plan) for plan in predict_seq]\n",
    "\n",
    "    elif ngram==2:\n",
    "        reference_ngram = [plan_to_bigram(plan) for plan in reference_seq]\n",
    "        prediction_ngram = [plan_to_bigram(plan) for plan in predict_seq]\n",
    "\n",
    "    elif ngram==3:\n",
    "        reference_ngram = [plan_to_trigram(plan) for plan in reference_seq]\n",
    "        prediction_ngram = [plan_to_trigram(plan) for plan in predict_seq]\n",
    "    else:\n",
    "        print('Wrong n-gram number. ')\n",
    "\n",
    "    average_match_rate = []\n",
    "    for ngram1, ngram2 in zip(reference_ngram, prediction_ngram):\n",
    "        ngram1_cnt = Counter(ngram1)\n",
    "        ngram2_cnt = Counter(ngram2)\n",
    "        match_cnt = 0\n",
    "        for ngram in ngram2_cnt.keys():\n",
    "            if ngram in ngram1_cnt:\n",
    "                # print(bigram1_cnt[bigram],bigram2_cnt[bigram])\n",
    "                # print(min(bigram1_cnt[bigram], bigram2_cnt[bigram]))\n",
    "                match_cnt += min(ngram1_cnt[ngram], ngram2_cnt[ngram])\n",
    "        if sum(ngram2_cnt.values()) != 0:\n",
    "            match_rate = match_cnt / sum(ngram2_cnt.values())\n",
    "            average_match_rate.append(match_rate)\n",
    "    print('Unigram match rates', np.mean(average_match_rate))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram match rates 0.676046819285604\n",
      "None\n",
      "Unigram match rates 0.3312019771473539\n",
      "None\n",
      "Unigram match rates 0.1311368567142325\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(n_gram_match_rate(test_predicted_plan, test_plan, ngram=1))\n",
    "print(n_gram_match_rate(test_predicted_plan, test_plan, ngram=2))\n",
    "print(n_gram_match_rate(test_predicted_plan, test_plan, ngram=3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 121084/121084 [10:41<00:00, 188.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the classifier on the test data: 0.9570099670526061\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from generator import StageClassifierModule\n",
    "\n",
    "stage_classifier = StageClassifierModule(classifier_path, device)\n",
    "\n",
    "correct_cnt = 0\n",
    "total_cnt = 0\n",
    "pbar = tqdm(total=len(test_plan))\n",
    "for text, reference_stages in zip(test_data, test_plan):\n",
    "    pbar.update(1)\n",
    "    _, text_list = extract_instruction(text, return_list=True)\n",
    "    if text_list!=[]:\n",
    "        predicted_stage = stage_classifier.compute_bert_stage_scores(text_list)\n",
    "        predicted_stage = torch.argmax(predicted_stage, dim=1)\n",
    "        for a, b in zip(predicted_stage, reference_stages):\n",
    "            if a==b:\n",
    "                correct_cnt+=1\n",
    "        total_cnt += len(reference_stages)\n",
    "pbar.close()\n",
    "\n",
    "accuracy = correct_cnt/total_cnt\n",
    "print('Accuracy of the classifier on the test data: {}'.format(accuracy))\n",
    "    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate recipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_dataset = RecipeInferenceDataset(\n",
    "                    {'text': test_data, \n",
    "                    'stage_label': test_predicted_plan\n",
    "                    }, \n",
    "                    tokenizer,\n",
    "                    max_length=512,\n",
    "                    use_special_token=True,\n",
    "                    with_stage_label=False\n",
    "                    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [05:40<00:00,  1.70s/it]\n"
     ]
    }
   ],
   "source": [
    "a, b = 0, 0.35\n",
    "evaluate_size = 200\n",
    "generation_doc, generation_doc_list = [], []\n",
    "reference_doc, reference_stage_label = [], []\n",
    "for i in tqdm(range(evaluate_size)):\n",
    "    datapoint = test_dataset[i]\n",
    "    stage_plan = datapoint['stage_label']\n",
    "    outputs = generator_model.structure_search(\n",
    "                    datapoint['input_ids'].to(device),\n",
    "                    beam_width=5,\n",
    "                    alpha=a,\n",
    "                    beta=b,\n",
    "                    stage_plan=stage_plan,\n",
    "                    max_length=512,\n",
    "                    )\n",
    "    generation = tokenizer.decode(outputs, skip_special_tokens=False )\n",
    "    generated_text, generated_text_list = extract_instruction(generation, return_list=True)\n",
    "    generation_doc.append(generated_text)\n",
    "    generation_doc_list.append(generated_text_list)\n",
    "    reference_doc.append(datapoint['reference_text'])\n",
    "    reference_stage_label.append(stage_plan)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU = 12.47 50.4/20.5/9.7/5.3 (BP = 0.824 ratio = 0.838 hyp_len = 17808 ref_len = 21258)\n",
      "Rouge-L Score: 0.3828653154102322\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(evaluate_fluency(generation_doc, reference_doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [01:43<00:00,  1.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0464485791985792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(compute_stage_matching(generation_doc, reference_stage_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2939293a08762870ec071caeb27575ab1d8f7475877917b6dc67f6d23d737eea"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
