{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import spacy\n",
    "import re\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from evaluation_utils import evaluate_fluency, compute_stage_matching, calculate_ingredient_coverage_and_hallucination\n",
    "from evaluation_utils import RecipeInferenceDataset, extract_instruction\n",
    "from dataclass import load_tokenizer\n",
    "from generator import RecipeGenerator\n",
    "from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "\n",
    "# from transformers import GPT2LMHeadModel, GPT2Tokenizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(121084, 121084)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "planner_result_path='/mnt/nas_home/yl535/RecipeWithPlans/model-checkpoint/planner_results/'\n",
    "with open(planner_result_path+'planner_prediction_test.json') as f:\n",
    "    test_predicted_plan = json.load(f)['planner_prediction']\n",
    "\n",
    "test_data_path='/mnt/nas_home/yl535/datasets/recipe1m+/preprocessed_data/test_dataset.json'\n",
    "with open(test_data_path) as f:\n",
    "    test_data = json.load(f)\n",
    "test_data, test_plan = test_data['text'], test_data['stage_label']\n",
    "\n",
    "len(test_predicted_plan), len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Can't load tokenizer for '/mnt/nas_home/yl535/RecipeWithPlans/model-checkpoint/generator_results/checkpoint-180000'. Make sure that:\n\n- '/mnt/nas_home/yl535/RecipeWithPlans/model-checkpoint/generator_results/checkpoint-180000' is a correct model identifier listed on 'https://huggingface.co/models'\n  (make sure '/mnt/nas_home/yl535/RecipeWithPlans/model-checkpoint/generator_results/checkpoint-180000' is not a path to a local directory with something else, in that case)\n\n- or '/mnt/nas_home/yl535/RecipeWithPlans/model-checkpoint/generator_results/checkpoint-180000' is the correct path to a directory containing relevant tokenizer files\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2214011/1701746316.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Load tokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# # Load planner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/RecipeWithPlans/dataclass.py\u001b[0m in \u001b[0;36mload_tokenizer\u001b[0;34m(tokenizer_name, stage_specified)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meos_token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGPT2Tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m                 \u001b[0mmsg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34mf\"- or '{revision}' is a valid git identifier (branch name, a tag name, or a commit id) that exists for this model name as listed on its model page on 'https://huggingface.co/models'\\n\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mEnvironmentError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mfile_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_path\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvocab_files\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Can't load tokenizer for '/mnt/nas_home/yl535/RecipeWithPlans/model-checkpoint/generator_results/checkpoint-180000'. Make sure that:\n\n- '/mnt/nas_home/yl535/RecipeWithPlans/model-checkpoint/generator_results/checkpoint-180000' is a correct model identifier listed on 'https://huggingface.co/models'\n  (make sure '/mnt/nas_home/yl535/RecipeWithPlans/model-checkpoint/generator_results/checkpoint-180000' is not a path to a local directory with something else, in that case)\n\n- or '/mnt/nas_home/yl535/RecipeWithPlans/model-checkpoint/generator_results/checkpoint-180000' is the correct path to a directory containing relevant tokenizer files\n\n"
     ]
    }
   ],
   "source": [
    "generator_path = '/mnt/nas_home/yl535/RecipeWithPlans/model-checkpoint/generator_results/checkpoint-150000'\n",
    "classifier_path = '/mnt/nas_home/yl535/RecipeWithPlans/model-checkpoint/classifier_results/checkpoint-45000'\n",
    "planner_path = '/mnt/nas_home/yl535/RecipeWithPlans/model-checkpoint/planner_results/checkpoint-8000'\n",
    "\n",
    "# Load device\n",
    "device = torch.device('cuda')\n",
    "\n",
    "# Load tokenizer \n",
    "tokenizer = load_tokenizer(generator_path)\n",
    "\n",
    "# # Load planner\n",
    "# planner_model = BartForConditionalGeneration.from_pretrained(planner_path)\n",
    "# bart_tok = BartTokenizer.from_pretrained(planner_path)\n",
    "\n",
    "# Load generator\n",
    "generator_model = RecipeGenerator(generator_path, \n",
    "                                tokenizer=tokenizer, \n",
    "                                device=device, \n",
    "                                classifier_path=classifier_path\n",
    "                                )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate classifier and planner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exact_match(predict_seq, reference_seq):\n",
    "    match_cnt = 0\n",
    "    total_cnt = 0\n",
    "    for predicted_plan, reference_plan in zip(predict_seq, reference_seq):\n",
    "        for p1, p2 in zip(predicted_plan, reference_plan):\n",
    "            if p1 and p2 and p1==p2:\n",
    "                match_cnt += 1\n",
    "\n",
    "        total_cnt += len(predicted_plan)\n",
    "    return match_cnt/total_cnt\n",
    "\n",
    "def plan_to_unigram(plan):\n",
    "    return [(stage) for stage in plan]\n",
    "    \n",
    "def plan_to_bigram(plan):\n",
    "    result = []\n",
    "    for i in range(len(plan)-1):\n",
    "        result.append(tuple(plan[i:i+2]))\n",
    "    return result\n",
    "\n",
    "def plan_to_trigram(plan):\n",
    "    result = []\n",
    "    for i in range(len(plan)-2):\n",
    "        result.append(tuple(plan[i:i+3]))\n",
    "    return result\n",
    "\n",
    "\n",
    "def n_gram_match_rate(predict_seq, reference_seq, ngram=1):\n",
    "    if ngram==1:\n",
    "        reference_ngram = [plan_to_unigram(plan) for plan in reference_seq]\n",
    "        prediction_ngram = [plan_to_unigram(plan) for plan in predict_seq]\n",
    "\n",
    "    elif ngram==2:\n",
    "        reference_ngram = [plan_to_bigram(plan) for plan in reference_seq]\n",
    "        prediction_ngram = [plan_to_bigram(plan) for plan in predict_seq]\n",
    "\n",
    "    elif ngram==3:\n",
    "        reference_ngram = [plan_to_trigram(plan) for plan in reference_seq]\n",
    "        prediction_ngram = [plan_to_trigram(plan) for plan in predict_seq]\n",
    "    else:\n",
    "        print('Wrong n-gram number. ')\n",
    "\n",
    "    average_match_rate = []\n",
    "    for ngram1, ngram2 in zip(reference_ngram, prediction_ngram):\n",
    "        ngram1_cnt = Counter(ngram1)\n",
    "        ngram2_cnt = Counter(ngram2)\n",
    "        match_cnt = 0\n",
    "        for ngram in ngram2_cnt.keys():\n",
    "            if ngram in ngram1_cnt:\n",
    "                # print(bigram1_cnt[bigram],bigram2_cnt[bigram])\n",
    "                # print(min(bigram1_cnt[bigram], bigram2_cnt[bigram]))\n",
    "                match_cnt += min(ngram1_cnt[ngram], ngram2_cnt[ngram])\n",
    "        if sum(ngram2_cnt.values()) != 0:\n",
    "            match_rate = match_cnt / sum(ngram2_cnt.values())\n",
    "            average_match_rate.append(match_rate)\n",
    "    print('Unigram match rates', np.mean(average_match_rate))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram match rates 0.676046819285604\n",
      "None\n",
      "Unigram match rates 0.3312019771473539\n",
      "None\n",
      "Unigram match rates 0.1311368567142325\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(n_gram_match_rate(test_predicted_plan, test_plan, ngram=1))\n",
    "print(n_gram_match_rate(test_predicted_plan, test_plan, ngram=2))\n",
    "print(n_gram_match_rate(test_predicted_plan, test_plan, ngram=3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 121084/121084 [10:41<00:00, 188.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the classifier on the test data: 0.9570099670526061\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from generator import StageClassifierModule\n",
    "\n",
    "stage_classifier = StageClassifierModule(classifier_path, device)\n",
    "\n",
    "correct_cnt = 0\n",
    "total_cnt = 0\n",
    "pbar = tqdm(total=len(test_plan))\n",
    "for text, reference_stages in zip(test_data, test_plan):\n",
    "    pbar.update(1)\n",
    "    _, text_list = extract_instruction(text, return_list=True)\n",
    "    if text_list!=[]:\n",
    "        predicted_stage = stage_classifier.compute_bert_stage_scores(text_list)\n",
    "        predicted_stage = torch.argmax(predicted_stage, dim=1)\n",
    "        for a, b in zip(predicted_stage, reference_stages):\n",
    "            if a==b:\n",
    "                correct_cnt+=1\n",
    "        total_cnt += len(reference_stages)\n",
    "pbar.close()\n",
    "\n",
    "accuracy = correct_cnt/total_cnt\n",
    "print('Accuracy of the classifier on the test data: {}'.format(accuracy))\n",
    "    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate recipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_dataset = RecipeInferenceDataset(\n",
    "                    {'text': test_data, \n",
    "                    'stage_label': test_predicted_plan\n",
    "                    }, \n",
    "                    tokenizer,\n",
    "                    max_length=512,\n",
    "                    use_special_token=True,\n",
    "                    with_stage_label=False\n",
    "                    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [52:59<00:00,  1.59s/it] \n"
     ]
    }
   ],
   "source": [
    "a, b = 0, 0.2\n",
    "evaluate_size = 2000\n",
    "generation_doc, generation_doc_list = [], []\n",
    "reference_doc, reference_stage_label = [], []\n",
    "for i in tqdm(range(evaluate_size)):\n",
    "    datapoint = test_dataset[i]\n",
    "    stage_plan = datapoint['stage_label']\n",
    "    outputs = generator_model.structure_search(\n",
    "                    datapoint['input_ids'].to(device),\n",
    "                    beam_width=5,\n",
    "                    alpha=a,\n",
    "                    beta=b,\n",
    "                    stage_plan=stage_plan,\n",
    "                    max_length=512,\n",
    "                    )\n",
    "    generation = tokenizer.decode(outputs, skip_special_tokens=False )\n",
    "    generated_text, generated_text_list = extract_instruction(generation, return_list=True)\n",
    "    generation_doc.append(generated_text)\n",
    "    generation_doc_list.append(generated_text_list)\n",
    "    reference_doc.append(datapoint['reference_text'])\n",
    "    reference_stage_label.append(stage_plan)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU = 13.73 53.4/22.4/11.4/6.6 (BP = 0.793 ratio = 0.811 hyp_len = 163941 ref_len = 202049)\n",
      "Rouge-L Score: 0.3910758962280766\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(evaluate_fluency(generation_doc, reference_doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:27<00:00, 72.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7567572441447442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(compute_stage_matching(generation_doc_list, reference_stage_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [58:52<00:00,  1.77s/it] \n"
     ]
    }
   ],
   "source": [
    "# Oracle version\n",
    "a, b = 0, 0.2\n",
    "evaluate_size = 2000\n",
    "generation_doc, generation_doc_list = [], []\n",
    "reference_doc, reference_stage_label = [], []\n",
    "for i in tqdm(range(evaluate_size)):\n",
    "    datapoint = test_dataset[i]\n",
    "    stage_plan = test_plan[i]\n",
    "    outputs = generator_model.structure_search(\n",
    "                    datapoint['input_ids'].to(device),\n",
    "                    beam_width=5,\n",
    "                    alpha=a,\n",
    "                    beta=b,\n",
    "                    stage_plan=stage_plan,\n",
    "                    max_length=512,\n",
    "                    )\n",
    "    generation = tokenizer.decode(outputs, skip_special_tokens=False )\n",
    "    generated_text, generated_text_list = extract_instruction(generation, return_list=True)\n",
    "    generation_doc.append(generated_text)\n",
    "    generation_doc_list.append(generated_text_list)\n",
    "    reference_doc.append(datapoint['reference_text'])\n",
    "    reference_stage_label.append(stage_plan)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU = 14.85 49.9/21.1/10.9/6.4 (BP = 0.902 ratio = 0.907 hyp_len = 183233 ref_len = 202049)\n",
      "Rouge-L Score: 0.3990109247049082\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:29<00:00, 67.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5987462468087469\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(evaluate_fluency(generation_doc, reference_doc))\n",
    "print(compute_stage_matching(generation_doc_list, reference_stage_label))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "94bf036f233ca66eed1fe5df02afb88e01a97bc35ef3dbbf8403762ef672bfc0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
